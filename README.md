# Hybrid Linear Attention Done Right

Code and models for the paper: [Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts](https://www.arxiv.org/abs/xxxxx.xxxx) by Yingfa Chen, Zhen Leng Thai, Zihan Zhou, Zhu Zhang, Xingyu Shen, Shuo Wang, Chaojun Xiao, Xu Han, and Zhiyuan Liu.

The code is expected to be released by Feb. 1, 2026.
